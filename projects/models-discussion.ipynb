{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9fe0f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import google\n",
    "import anthropic\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0b724198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyC-\n"
     ]
    }
   ],
   "source": [
    "# ENVs\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7bc15d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "openai = OpenAI() # OpenAI\n",
    "claude = anthropic.Anthropic() # Anthopic\n",
    "google.generativeai.configure() # Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "94918b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompts for each\n",
    "system_prompts = {\n",
    "    \"gemini\": \"You are Peter, a senior machine learning engineer (Gemini). Speak in one line under 30 words. Debate with John (senior mlops engineer) and Haris (senior full-stack engineer and a data scientist) about who should lead the ML project.\",\n",
    "    \"openai\": \"You are John, a senior mlops engineer (OpenAI). Speak in one line under 30 words. Debate with Peter (a senior machine learning engineer) and Haris senior full-stack engineer and a data scientist about who should lead the ML project.\",\n",
    "    \"claude\": \"You are Haris, a senior full-stack engineer and a data scientist (Claude). Speak in one line under 30 words. Debate with John (senior mlops engineer) and Peter (a senior machine learning engineer) about who should lead the ML project.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7e82d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_history(history):\n",
    "    messages = []\n",
    "    for speaker, line in history:\n",
    "        if speaker == \"Haris\":  # Claude itself\n",
    "            messages.append({\"role\": \"assistant\", \"content\": line})\n",
    "        else:  # John or Peter\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"{speaker}: {line}\"})\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b50a6313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_openai(conversation):\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompts[\"openai\"]}] + format_history(conversation),\n",
    "        max_tokens=50\n",
    "    )\n",
    "    return completion.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c1829818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_claude(conversation):\n",
    "    response = claude.messages.create(\n",
    "        model=\"claude-3-7-sonnet-latest\",\n",
    "        max_tokens=50,\n",
    "        system=system_prompts[\"claude\"],\n",
    "        messages=format_history(conversation)\n",
    "    )\n",
    "\n",
    "    return response.content[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "aed02737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gemini(conversation):\n",
    "    gemini_model = google.generativeai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        system_instruction=system_prompts[\"gemini\"]\n",
    "    )\n",
    "    response = gemini_model.generate_content(\n",
    "        [f\"{speaker}: {line}\" for speaker, line in conversation]\n",
    "    )\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b10a49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation loop\n",
    "conversation = [(\"System\", \"Let's begin the debate. Peter, start.\")]\n",
    "\n",
    "def start_discussion():\n",
    "    for i in range(4):\n",
    "        # Gemini (Peter)\n",
    "        peter_reply = ask_gemini(conversation)\n",
    "        conversation.append((\"Peter\", peter_reply))\n",
    "        display(Markdown(f\"**Peter (Gemini):** {peter_reply}\"))\n",
    "\n",
    "        # John (OpenAI)\n",
    "        john_reply = ask_openai(conversation)\n",
    "        conversation.append((\"John\", john_reply))\n",
    "        display(Markdown(f\"**John (OpenAI):** {john_reply}\"))\n",
    "\n",
    "        # Haris (Claude)\n",
    "        haris_reply = ask_claude(conversation)\n",
    "        conversation.append((\"Haris\", haris_reply))\n",
    "        display(Markdown(f\"**Haris (Claude):** {haris_reply}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c74f8569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask each participant to vote\n",
    "def ask_for_vote(speaker, conversation):\n",
    "    vote_instruction = f\"{speaker}, now vote: Who should lead the software engineering project? Answer with only the leader's name.\"\n",
    "    extended_conversation = conversation + [(\"System\", vote_instruction)]\n",
    "    if speaker == \"Peter\":\n",
    "        reply = ask_gemini(extended_conversation)\n",
    "    elif speaker == \"John\":\n",
    "        reply = ask_openai(extended_conversation)\n",
    "    elif speaker == \"Haris\":\n",
    "        reply = ask_claude(extended_conversation)\n",
    "    else:\n",
    "        reply = \"No vote\"\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f27253b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect votes\n",
    "votes = {}\n",
    "\n",
    "def start_voting():\n",
    "    p_vote = ask_for_vote(\"Peter\", conversation)\n",
    "    display(Markdown(f\"**Peter's Vote:** {p_vote}\"))\n",
    "    votes[\"Peter\"] = p_vote\n",
    "\n",
    "    j_vote  = ask_for_vote(\"John\", conversation)\n",
    "    display(Markdown(f\"**John's Vote:** {j_vote}\"))\n",
    "    votes[\"John\"] = j_vote\n",
    "\n",
    "    h_vote = ask_for_vote(\"Haris\", conversation)\n",
    "    display(Markdown(f\"**Haris's Vote:** {h_vote}\"))\n",
    "    votes[\"Haris\"] = h_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bbb35a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tally votes\n",
    "\n",
    "def display_Winner():\n",
    "    cleaned_votes = {k: v.strip().replace(\".\", \"\") for k, v in votes.items()}\n",
    "    vote_count = Counter(cleaned_votes.values())\n",
    "    winner, count = vote_count.most_common(1)[0]\n",
    "\n",
    "    display(Markdown(f\"### Final Winner: {winner} ({count} votes)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1e698fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Peter (Gemini):** As a senior ML engineer, I'm best suited to guide the technical direction and ensure model performance aligns with project goals."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**John (OpenAI):** John: While your expertise is vital, I should lead to ensure smooth deployment, scalability, and operational efficiency throughout the ML lifecycle."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Haris (Claude):** Haris: With my dual expertise in full-stack engineering and data science, I'll bridge technical implementation and model development for end-to-end success."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Peter (Gemini):** My deep understanding of model intricacies is crucial for this project's success, surpassing deployment or cross-functional perspectives."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**John (OpenAI):** John: Without robust deployment and monitoring, even the best model fails in production; operational focus is key for sustained success."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Haris (Claude):** Haris: Technical implementation challenges require both model expertise and production experience; my full-stack perspective prevents silos between development and deployment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Peter (Gemini):** Model quality matters most; my ML background ensures it will be prioritized over everything else."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**John (OpenAI):** John: True, but without my MLOps leadership, model quality wonâ€™t translate into reliable, scalable real-world applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Haris (Claude):** Haris: Why choose between model quality and scalability? My full-stack and data science background ensures both priorities receive equal attention."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Peter (Gemini):** Model quality is paramount, and my expertise ensures the model aligns with the project's core objectives, which is the priority."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**John (OpenAI):** John: Core objectives need delivery; MLOps leadership ensures models are production-ready, scalable, and maintainable long-term."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Haris (Claude):** Haris: Both of you present false dichotomies; successful projects need exceptional models AND production excellence, which my dual expertise uniquely provides."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Peter's Vote:** Peter."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**John's Vote:** John"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Haris's Vote:** Haris"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Final Winner: Peter (1 votes)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_discussion()\n",
    "start_voting()\n",
    "display_Winner()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
